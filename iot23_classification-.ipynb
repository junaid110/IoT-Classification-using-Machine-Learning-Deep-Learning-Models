{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and EDA for IoT23 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('preprocessed_iot23.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ts</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>local_orig</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>proto</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>POHScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>POHScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>POHScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>POHScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>8081.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>POHScan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            ts  id.orig_p  id.resp_p  duration  orig_bytes  \\\n",
       "0           0  1.536227e+09    17576.0     8081.0  0.000003         0.0   \n",
       "1           1  1.536227e+09    17576.0     8081.0  0.000002         0.0   \n",
       "2           2  1.536227e+09    17576.0     8081.0  0.000002         0.0   \n",
       "3           3  1.536227e+09    17576.0     8081.0  0.000002         0.0   \n",
       "4           4  1.536227e+09    17576.0     8081.0  0.000002         0.0   \n",
       "\n",
       "   resp_bytes  local_orig  local_resp  missed_bytes  orig_pkts  orig_ip_bytes  \\\n",
       "0         0.0           0           0           0.0        2.0           80.0   \n",
       "1         0.0           0           0           0.0        2.0           80.0   \n",
       "2         0.0           0           0           0.0        2.0           80.0   \n",
       "3         0.0           0           0           0.0        2.0           80.0   \n",
       "4         0.0           0           0           0.0        2.0           80.0   \n",
       "\n",
       "   resp_pkts  resp_ip_bytes  proto  conn_state    label  \n",
       "0        0.0            0.0      1           6  POHScan  \n",
       "1        0.0            0.0      1           6  POHScan  \n",
       "2        0.0            0.0      1           6  POHScan  \n",
       "3        0.0            0.0      1           6  POHScan  \n",
       "4        0.0            0.0      1           6  POHScan  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>local_orig</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>proto</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1215130</th>\n",
       "      <td>1.562165e+09</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>90.034713</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215131</th>\n",
       "      <td>1.562165e+09</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.399970</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215132</th>\n",
       "      <td>1.562165e+09</td>\n",
       "      <td>135.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>89.824030</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215133</th>\n",
       "      <td>1.562165e+09</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.215915</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215134</th>\n",
       "      <td>1.562165e+09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>44.242223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ts  id.orig_p  id.resp_p   duration  orig_bytes  \\\n",
       "1215130  1.562165e+09       68.0       67.0  90.034713      3300.0   \n",
       "1215131  1.562165e+09      143.0        0.0  90.399970       340.0   \n",
       "1215132  1.562165e+09      135.0      136.0  89.824030        72.0   \n",
       "1215133  1.562165e+09      143.0        0.0  45.215915       200.0   \n",
       "1215134  1.562165e+09      133.0      134.0  44.242223         0.0   \n",
       "\n",
       "         resp_bytes  local_orig  local_resp  missed_bytes  orig_pkts  \\\n",
       "1215130         0.0           0           0           0.0       11.0   \n",
       "1215131         0.0           0           0           0.0        9.0   \n",
       "1215132         0.0           0           0           0.0        3.0   \n",
       "1215133         0.0           0           0           0.0        8.0   \n",
       "1215134         0.0           0           0           0.0        2.0   \n",
       "\n",
       "         orig_ip_bytes  resp_pkts  resp_ip_bytes  proto  conn_state   label  \n",
       "1215130         3608.0        0.0            0.0      2           6  Benign  \n",
       "1215131          844.0        0.0            0.0      0           0  Benign  \n",
       "1215132          216.0        0.0            0.0      0           0  Benign  \n",
       "1215133          648.0        0.0            0.0      0           0  Benign  \n",
       "1215134           96.0        0.0            0.0      0           0  Benign  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1215135, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ts', 'id.orig_p', 'id.resp_p', 'duration', 'orig_bytes', 'resp_bytes',\n",
      "       'local_orig', 'local_resp', 'missed_bytes', 'orig_pkts',\n",
      "       'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto', 'conn_state',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop port numbers because they can cause overfitting of the model\n",
    "df= data.drop(columns=['id.orig_p','id.resp_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the X variables\n",
    "X=df.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate labels (y)\n",
    "y=data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>local_orig</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>proto</th>\n",
       "      <th>conn_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1215135.0</td>\n",
       "      <td>1215135.0</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "      <td>1.215135e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.544026e+09</td>\n",
       "      <td>4.160833e-01</td>\n",
       "      <td>1.744161e+03</td>\n",
       "      <td>2.845327e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.682529e-02</td>\n",
       "      <td>2.097919e+02</td>\n",
       "      <td>7.640348e+03</td>\n",
       "      <td>2.780588e-01</td>\n",
       "      <td>2.992224e+02</td>\n",
       "      <td>1.094919e+00</td>\n",
       "      <td>4.509022e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.586241e+07</td>\n",
       "      <td>9.890378e+01</td>\n",
       "      <td>9.928449e+05</td>\n",
       "      <td>3.053014e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.522650e+00</td>\n",
       "      <td>7.882927e+04</td>\n",
       "      <td>2.652670e+06</td>\n",
       "      <td>2.174025e+02</td>\n",
       "      <td>3.171909e+05</td>\n",
       "      <td>3.141910e-01</td>\n",
       "      <td>2.660314e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.533077e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.536227e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.545398e+09</td>\n",
       "      <td>2.000000e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.569018e+09</td>\n",
       "      <td>7.884033e+04</td>\n",
       "      <td>9.623472e+08</td>\n",
       "      <td>3.365164e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.363000e+03</td>\n",
       "      <td>6.602735e+07</td>\n",
       "      <td>1.914793e+09</td>\n",
       "      <td>2.394840e+05</td>\n",
       "      <td>3.496187e+08</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.200000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ts      duration    orig_bytes    resp_bytes  local_orig  \\\n",
       "count  1.215135e+06  1.215135e+06  1.215135e+06  1.215135e+06   1215135.0   \n",
       "mean   1.544026e+09  4.160833e-01  1.744161e+03  2.845327e+02         0.0   \n",
       "std    1.586241e+07  9.890378e+01  9.928449e+05  3.053014e+05         0.0   \n",
       "min    1.525880e+09  0.000000e+00  0.000000e+00  0.000000e+00         0.0   \n",
       "25%    1.533077e+09  0.000000e+00  0.000000e+00  0.000000e+00         0.0   \n",
       "50%    1.536227e+09  0.000000e+00  0.000000e+00  0.000000e+00         0.0   \n",
       "75%    1.545398e+09  2.000000e-06  0.000000e+00  0.000000e+00         0.0   \n",
       "max    1.569018e+09  7.884033e+04  9.623472e+08  3.365164e+08         0.0   \n",
       "\n",
       "       local_resp  missed_bytes     orig_pkts  orig_ip_bytes     resp_pkts  \\\n",
       "count   1215135.0  1.215135e+06  1.215135e+06   1.215135e+06  1.215135e+06   \n",
       "mean          0.0  1.682529e-02  2.097919e+02   7.640348e+03  2.780588e-01   \n",
       "std           0.0  8.522650e+00  7.882927e+04   2.652670e+06  2.174025e+02   \n",
       "min           0.0  0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00   \n",
       "25%           0.0  0.000000e+00  1.000000e+00   4.000000e+01  0.000000e+00   \n",
       "50%           0.0  0.000000e+00  1.000000e+00   4.000000e+01  0.000000e+00   \n",
       "75%           0.0  0.000000e+00  2.000000e+00   8.000000e+01  0.000000e+00   \n",
       "max           0.0  7.363000e+03  6.602735e+07   1.914793e+09  2.394840e+05   \n",
       "\n",
       "       resp_ip_bytes         proto    conn_state  \n",
       "count   1.215135e+06  1.215135e+06  1.215135e+06  \n",
       "mean    2.992224e+02  1.094919e+00  4.509022e+00  \n",
       "std     3.171909e+05  3.141910e-01  2.660314e+00  \n",
       "min     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "25%     0.000000e+00  1.000000e+00  0.000000e+00  \n",
       "50%     0.000000e+00  1.000000e+00  6.000000e+00  \n",
       "75%     0.000000e+00  1.000000e+00  6.000000e+00  \n",
       "max     3.496187e+08  2.000000e+00  1.200000e+01  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts               0\n",
       "duration         0\n",
       "orig_bytes       0\n",
       "resp_bytes       0\n",
       "local_orig       0\n",
       "local_resp       0\n",
       "missed_bytes     0\n",
       "orig_pkts        0\n",
       "orig_ip_bytes    0\n",
       "resp_pkts        0\n",
       "resp_ip_bytes    0\n",
       "proto            0\n",
       "conn_state       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          POHScan\n",
       "1          POHScan\n",
       "2          POHScan\n",
       "3          POHScan\n",
       "4          POHScan\n",
       "            ...   \n",
       "1215130     Benign\n",
       "1215131     Benign\n",
       "1215132     Benign\n",
       "1215133     Benign\n",
       "1215134     Benign\n",
       "Name: label, Length: 1215135, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Okiru      374621\n",
       "DDoS       374204\n",
       "POHScan    313027\n",
       "Benign     145058\n",
       "C&C          8225\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the labels are encoded in alphabetic order\n",
    "#0---->Benign\n",
    "#1---->Cnc\n",
    "#2---->DDoS\n",
    "#3---->Okiru\n",
    "#4---->POHScan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POHScan', 'POHScan', 'POHScan', ..., 'Benign', 'Benign', 'Benign'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1215135,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import auc,roc_auc_score,roc_curve,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=124, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = std_scale.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std  = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972108, 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training within 47.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train_std, Y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Finished training within {:.2f} seconds\".format(elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for AdaBoost: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.90      0.45     28695\n",
      "           1       0.00      0.00      0.00      1626\n",
      "           2       0.02      0.00      0.00     74607\n",
      "           3       1.00      0.80      0.89     75258\n",
      "           4       0.64      0.98      0.78     62841\n",
      "\n",
      "    accuracy                           0.61    243027\n",
      "   macro avg       0.39      0.54      0.42    243027\n",
      "weighted avg       0.52      0.61      0.53    243027\n",
      "\n",
      "Confusion matrix for AdaBoost: \n",
      "[[25801     0   254     2  2638]\n",
      " [    1     0     0     0  1625]\n",
      " [59491     0     6     0 15110]\n",
      " [   10     0     3 60206 15039]\n",
      " [ 1179     0     0     0 61662]]\n",
      "Accuracy score for AdaBoost: 0.6076\n",
      "Precision score for AdaBoost: 0.5178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for AdaBoost: 0.6076\n",
      "F1 score for AdaBoost: 0.5289\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calc_sens_spec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 score for AdaBoost: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f1_adaboost))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Calculate sensitivity, specificity, and AUC for AdaBoost\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m sens_adaboost, spec_adaboost \u001b[38;5;241m=\u001b[39m calc_sens_spec(Y_test, y_adaboost)\n\u001b[0;32m     29\u001b[0m adaboost_fpr, adaboost_tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(Y_test, y_adaboost_prob[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     30\u001b[0m auc_adaboost \u001b[38;5;241m=\u001b[39m roc_auc_score(Y_test, y_adaboost_prob[:,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calc_sens_spec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict with AdaBoost model\n",
    "y_adaboost = adaboost_model.predict(X_test_std)\n",
    "y_adaboost_prob = adaboost_model.predict_proba(X_test_std)\n",
    "\n",
    "# Classification report for AdaBoost\n",
    "print(\"Classification report for AdaBoost: \\n{}\".format(classification_report(Y_test, y_adaboost)))\n",
    "\n",
    "# Confusion matrix for AdaBoost\n",
    "print(\"Confusion matrix for AdaBoost: \\n{}\".format(confusion_matrix(Y_test, y_adaboost)))\n",
    "\n",
    "# Accuracy score for AdaBoost\n",
    "accuracy_adaboost = accuracy_score(Y_test, y_adaboost)\n",
    "print(\"Accuracy score for AdaBoost: {:.4f}\".format(accuracy_adaboost))\n",
    "\n",
    "# Precision score for AdaBoost\n",
    "prec_adaboost = precision_score(Y_test, y_adaboost,  average='weighted')\n",
    "print(\"Precision score for AdaBoost: {:.4f}\".format(prec_adaboost))\n",
    "\n",
    "# Recall score for AdaBoost\n",
    "rec_adaboost = recall_score(Y_test, y_adaboost,  average='weighted')\n",
    "print(\"Recall score for AdaBoost: {:.4f}\".format(rec_adaboost))\n",
    "\n",
    "# F1 score for AdaBoost\n",
    "f1_adaboost = f1_score(Y_test, y_adaboost,  average='weighted')\n",
    "print(\"F1 score for AdaBoost: {:.4f}\".format(f1_adaboost))\n",
    "\n",
    "# Calculate sensitivity, specificity, and AUC for AdaBoost\n",
    "sens_adaboost, spec_adaboost = calc_sens_spec(Y_test, y_adaboost)\n",
    "adaboost_fpr, adaboost_tpr, _ = roc_curve(Y_test, y_adaboost_prob[:,1])\n",
    "auc_adaboost = roc_auc_score(Y_test, y_adaboost_prob[:,1])\n",
    "print(\"Sensitivity score for AdaBoost: {:.2f}\".format(sens_adaboost))\n",
    "print(\"Specificity score for AdaBoost: {:.2f}\".format(spec_adaboost))\n",
    "print(\"AUC score for AdaBoost: {:.2f}\".format(auc_adaboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training AdaBoost Classifier within 46.19 seconds\n",
      "Accuracy for class Benign: 0.8991\n",
      "Accuracy for class C&C: 0.0000\n",
      "Accuracy for class DDoS: 0.0001\n",
      "Accuracy for class Okiru: 0.8000\n",
      "Accuracy for class POHScan: 0.9812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# Assuming you have defined X_train_std, Y_train, X_test_std, Y_test\n",
    "# Also, assuming you have a LabelEncoder named le\n",
    "\n",
    "# Create a base classifier (you can use any classifier here, e.g., DecisionTreeClassifier)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create the AdaBoostClassifier\n",
    "adaboost_model = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Training the AdaBoostClassifier\n",
    "start_time_adaboost = time.time()\n",
    "adaboost_model.fit(X_train_std, Y_train)\n",
    "end_time_adaboost = time.time()\n",
    "\n",
    "elapsed_time_adaboost = end_time_adaboost - start_time_adaboost\n",
    "print(\"Finished training AdaBoost Classifier within {:.2f} seconds\".format(elapsed_time_adaboost))\n",
    "\n",
    "# Prediction with the AdaBoost Classifier\n",
    "y_adaboost = adaboost_model.predict(X_test_std)\n",
    "\n",
    "# Calculate the confusion matrix for AdaBoost predictions\n",
    "cf_adaboost = confusion_matrix(Y_test, y_adaboost)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "accuracies_adaboost = cf_adaboost.diagonal() / cf_adaboost.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class\n",
    "classes_adaboost = le.classes_  # Assuming le is your LabelEncoder\n",
    "for class_label, accuracy in zip(classes_adaboost, accuracies_adaboost):\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer.time()\n",
    "\n",
    "# Create a CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_seed=124, verbose=200)\n",
    "\n",
    "# Fit the model to the training data\n",
    "catboost_model.fit(X_train_std, Y_train)\n",
    "\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set using the trained CatBoost model\n",
    "y_catboost = catboost_model.predict(X_test_std)\n",
    "\n",
    "# Predict the class probabilities for the test set\n",
    "y_catboost_prob = catboost_model.predict_proba(X_test_std)\n",
    "\n",
    "# The 'y_catboost' variable contains the predicted labels, and 'y_catboost_prob' contains the class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate predictions using the CatBoost model\n",
    "y_catboost = catboost_model.predict(X_test_std)\n",
    "\n",
    "# Calculate the predicted class probabilities\n",
    "y_catboost_prob = catboost_model.predict_proba(X_test_std)\n",
    "\n",
    "# Convert predicted labels to integers\n",
    "y_catboost = y_catboost.astype(int)\n",
    "\n",
    "# Calculate and print the classification report\n",
    "classification_rep = classification_report(Y_test, y_catboost, target_names=le.classes_)\n",
    "print(\"Classification report for CatBoost:\\n\", classification_rep)\n",
    "\n",
    "# Calculate and print the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_test, y_catboost)\n",
    "print(\"Confusion matrix for CatBoost:\\n\", confusion_mtx)\n",
    "\n",
    "# Calculate and print the accuracy score\n",
    "accuracy = accuracy_score(Y_test, y_catboost)\n",
    "print(\"Accuracy score for CatBoost: {:.4f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print precision, recall, and F1 scores\n",
    "prec_catboost = precision_score(Y_test, y_catboost, average='weighted')\n",
    "rec_catboost = recall_score(Y_test, y_catboost, average='weighted')\n",
    "f1_catboost = f1_score(Y_test, y_catboost, average='weighted')\n",
    "print(\"Precision score for CatBoost: {:.4f}\".format(prec_catboost))\n",
    "\n",
    "recall_catboost = recall_score(Y_test, y_catboost, average='weighted')\n",
    "print(\"Recall score for CatBoost: {:.4f}\".format(recall_catboost))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_catboost = f1_score(Y_test, y_catboost, average='weighted')\n",
    "print(\"F1 score for CatBoost: {:.4f}\".format(f1_catboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate predictions using the CatBoost model\n",
    "y_catboost = catboost_model.predict(X_test_std)\n",
    "\n",
    "# Calculate the confusion matrix for CatBoost predictions\n",
    "cf_catboost = confusion_matrix(Y_test, y_catboost)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "accuracies = cf_catboost.diagonal() / cf_catboost.sum(axis=1)\n",
    "\n",
    "# Now, 'accuracies' saves the accuracy for each class\n",
    "\n",
    "# Print the accuracy for each class\n",
    "classes = le.classes_  # Assuming le is your LabelEncoder\n",
    "for class_label, accuracy in zip(classes, accuracies):\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate predictions using the CatBoost model\n",
    "y_catboost = catboost_model.predict(X_test_std)\n",
    "\n",
    "# Calculate the confusion matrix for CatBoost predictions\n",
    "cf_catboost = confusion_matrix(Y_test, y_catboost)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "accuracies = cf_catboost.diagonal() / cf_catboost.sum(axis=1)\n",
    "\n",
    "# Get class labels from the LabelEncoder\n",
    "classes = le.classes_\n",
    "\n",
    "# Create a pie chart to visualize the accuracy for each class\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(accuracies, labels=classes, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Accuracy for Each Class')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.savefig(r'C:\\Users\\Junaid Abbas\\Desktop\\Research Project\\Iot_23\\pictures\\accuracy_pie_chart.png')\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the FNN model\n",
    "model = keras.Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_std.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the FNN model\n",
    "start = timer.time()\n",
    "model.fit(X_train_std, Y_train, epochs=10, batch_size=32, verbose=1)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the FNN model\n",
    "y_fnn_prob = model.predict(X_test_std)\n",
    "\n",
    "# Convert predicted probabilities to binary class \n",
    "y_fnn = (y_fnn_prob > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict with the FNN model\n",
    "y_fnn_prob = model.predict(X_test_std)\n",
    "\n",
    "# Convert predicted probabilities to binary class labels (0 or 1)\n",
    "y_fnn = (y_fnn_prob > 0.5).astype(int)\n",
    "\n",
    "# Classification report, confusion matrix, and accuracy score for FNN\n",
    "print(\"Classification report for FNN: \\n{}\".format(classification_report(Y_test, y_fnn)))\n",
    "print(\"Confusion matrix for FNN: \\n{}\".format(confusion_matrix(Y_test, y_fnn)))\n",
    "print(\"Accuracy score for FNN: {:.4f}\".format(accuracy_score(Y_test, y_fnn)))\n",
    "\n",
    "# Calculate precision, recall, and F1 scores for FNN\n",
    "prec_fnn = precision_score(Y_test, y_fnn, average='weighted')\n",
    "rec_fnn = recall_score(Y_test, y_fnn, average='weighted')\n",
    "f1_fnn = f1_score(Y_test, y_fnn, average='weighted')\n",
    "print(\"Precision score for FNN: {:.4f}\".format(prec_fnn))\n",
    "print(\"Recall score for FNN: {:.4f}\".format(rec_fnn))\n",
    "print(\"F1 score for FNN: {:.4f}\".format(f1_fnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the confusion matrix for FNN\n",
    "cf = confusion_matrix(Y_test, y_fnn)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "class_accuracy = np.diag(cf) / cf.sum(axis=1)\n",
    "\n",
    "# Print the array of class accuracies\n",
    "print(class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "class_labels = le.classes_  # Assuming le is the LabelEncoder you used\n",
    "\n",
    "# Create a variable to store class accuracies\n",
    "class_accuracies = {}\n",
    "\n",
    "for label, accuracy in zip(class_labels, class_accuracy):\n",
    "    class_accuracies[label] = accuracy\n",
    "\n",
    "# Print the class accuracies\n",
    "for label, accuracy in class_accuracies.items():\n",
    "    print(f\"Class: {label}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you already have the class_accuracies dictionary\n",
    "\n",
    "# Get class labels and accuracies\n",
    "class_labels = list(class_accuracies.keys())\n",
    "accuracies = list(class_accuracies.values())\n",
    "\n",
    "# Create a pie chart to visualize the accuracies for each class\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(accuracies, labels=class_labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Accuracy for Each Class')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.\n",
    "\n",
    "# Save the pie chart as an image\n",
    "plt.savefig(r'C:\\Users\\Junaid Abbas\\Desktop\\Research Project\\Iot_23\\pictures\\class_accuracy_pie_chart.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Create an LDA model\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "start = timer.time()\n",
    "\n",
    "# Fit the LDA model to the training data\n",
    "lda_model.fit(X_train_std, Y_train)\n",
    "\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the LDA model\n",
    "y_lda = lda_model.predict(X_test_std)\n",
    "\n",
    "# Get class probabilities\n",
    "y_lda_prob = lda_model.predict_proba(X_test_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate predictions using the LDA model\n",
    "y_lda = lda_model.predict(X_test_std)\n",
    "\n",
    "# Generate a classification report\n",
    "classification_rep_lda = classification_report(Y_test, y_lda)\n",
    "\n",
    "# Calculate the confusion matrix for LDA predictions\n",
    "cf_lda = confusion_matrix(Y_test, y_lda)\n",
    "\n",
    "# Calculate the accuracy score for LDA predictions\n",
    "accuracy_lda = accuracy_score(Y_test, y_lda)\n",
    "\n",
    "# Calculate precision, recall, and F1 scores for LDA\n",
    "prec_lda = precision_score(Y_test, y_lda, average='weighted')\n",
    "rec_lda = recall_score(Y_test, y_lda, average='weighted')\n",
    "f1_lda = f1_score(Y_test, y_lda, average='weighted')\n",
    "\n",
    "print(\"Classification report for LDA: \\n{}\".format(classification_rep_lda))\n",
    "print(\"Confusion matrix for LDA: \\n{}\".format(cf_lda))\n",
    "print(\"Accuracy score for LDA: {:.4f}\".format(accuracy_lda))\n",
    "print(\"Precision score for LDA: {:.4f}\".format(prec_lda))\n",
    "print(\"Recall score for LDA: {:.4f}\".format(rec_lda))\n",
    "print(\"F1 score for LDA: {:.4f}\".format(f1_lda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Perform LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, Y_train)\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "\n",
    "# Create a Logistic Regression classifier\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_lda, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_regression.predict(X_test_lda)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "class_accuracy = confusion.diagonal() / confusion.sum(axis=1)\n",
    "\n",
    "# Print the array of class accuracies\n",
    "print(\"Class Accuracies:\", class_accuracy)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) itself does not produce a confusion matrix, but it is often used as a preprocessing step for classification. To create a confusion matrix, you need to use the LDA-transformed features as input to a classification model and then evaluate the model's performance. Here's how you can adapt the code to work with LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already calculated and stored class_accuracy as a list\n",
    "# The class names are stored in the \"class_names\" list\n",
    "class_names = [\"Benign\", \"DDoS\", \"C&C\", \"POHScan\", \"Okiru\"]\n",
    "# Create a pie chart to visualize the accuracy for each class\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(class_accuracy, labels=class_names, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Accuracy for Each Class')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular\n",
    "\n",
    "plt.savefig(r'C:\\Users\\Junaid Abbas\\Desktop\\Research Project\\Iot_23\\pictures\\LDA_class_accuracy_pie_chart.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stacking, you train multiple models and use a meta-model to combine their predictions. Here, I'll demonstrate stacking with a base logistic regression model and a meta-model, typically a simpler model like another logistic regression or any other classifier. You can extend this example with more diverse base models based on your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming you have X_train_std, Y_train prepared\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_std, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "base_model = LogisticRegression(solver='sag', max_iter=300, multi_class='multinomial')\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[('base_model', base_model)],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # You can adjust the number of folds for cross-validation\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "start = time.time()\n",
    "stacking_model.fit(X_train_scaled, Y_train)\n",
    "end = time.time()\n",
    "print(\"Finished training stacking model within {:.2f} seconds\".format(end - start))\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = stacking_model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Classification report for Stacking: \\n{}\".format(classification_report(Y_val, y_val_pred)))\n",
    "print(\"Confusion matrix for Stacking: \\n{}\".format(confusion_matrix(Y_val, y_val_pred)))\n",
    "print(\"Accuracy score for Stacking: {:.4f}\".format(accuracy_score(Y_val, y_val_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the confusion matrix for stacking model cf_stacking\n",
    "cf_stacking = np.array([[20987, 0, 102, 149, 2082],\n",
    "                        [1, 0, 0, 0, 1287],\n",
    "                        [0, 0, 47787, 0, 11912],\n",
    "                        [0, 0, 9, 48048, 11959],\n",
    "                        [304, 0, 0, 0, 49795]])\n",
    "\n",
    "# Calculate accuracy for each class in stacking model\n",
    "class_accuracy_stacking = cf_stacking.diagonal() / cf_stacking.sum(axis=1)\n",
    "\n",
    "# Print accuracy for each class in stacking model\n",
    "for i, acc in enumerate(class_accuracy_stacking):\n",
    "    print(f'Accuracy for Class {i}: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create a base Gaussian Naive Bayes model\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# Create a BaggingClassifier using the Gaussian Naive Bayes model\n",
    "bagging_model = BaggingClassifier(base_estimator=gnb_model, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "start = timer.time()\n",
    "bagging_model.fit(X_train_std, Y_train)\n",
    "end = timer.time()\n",
    "print(\"Finished training within {:.2f} seconds\".format(end - start))\n",
    "\n",
    "# Predictions\n",
    "bagging_predictions = bagging_model.predict(X_test_std)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"\\nBagging Classifier with Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, bagging_predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, bagging_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=124, test_size=0.20, shuffle=True)\n",
    "\n",
    "# Standardize the features\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)\n",
    "print(\"X_train_std shape:\", X_train_std.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Number of base CatBoost models in the ensemble\n",
    "num_models = 4  # You can adjust this number\n",
    "\n",
    "# List to store individual CatBoost models\n",
    "catboost_models = []\n",
    "\n",
    "# Train individual CatBoost models\n",
    "for i in range(num_models):\n",
    "    indices = np.random.choice(len(X_train_std), len(X_train_std), replace=True)\n",
    "    catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_seed=124, verbose=200)\n",
    "    catboost_model.fit(X_train_std[indices], Y_train[indices])\n",
    "    catboost_models.append(catboost_model)\n",
    "\n",
    "# Make predictions using individual models\n",
    "individual_predictions = [model.predict(X_test_std) for model in catboost_models]\n",
    "\n",
    "# Create an ensemble prediction by majority voting\n",
    "ensemble_predictions = np.round(np.mean(individual_predictions, axis=0))\n",
    "\n",
    "# Calculate the confusion matrix for ensemble predictions\n",
    "cf_ensemble = confusion_matrix(Y_test, ensemble_predictions)\n",
    "\n",
    "# Calculate the accuracy for each class\n",
    "accuracies_ensemble = cf_ensemble.diagonal() / cf_ensemble.sum(axis=1)\n",
    "\n",
    "# Print the accuracy for each class\n",
    "classes = le.classes_  # Assuming le is your LabelEncoder\n",
    "for class_label, accuracy in zip(classes, accuracies_ensemble):\n",
    "    print(f\"Accuracy for class {class_label}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
